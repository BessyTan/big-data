{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce07ecf-6eef-4764-a63e-78a45e937a94",
   "metadata": {},
   "source": [
    "# Create SparkContext & SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c68f495-6628-429c-b844-e5894b394340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/28 17:17:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa12956-f379-4818-bf89-d00f29b45252",
   "metadata": {},
   "source": [
    "# TF, IDF and TF-IDF\n",
    "* TF is short for **Term Frequency**.\n",
    "  It is simply the frequency of a term in a document.\n",
    "  The higher the TF is for a specific term, the more important that term is to the document\n",
    "* IDF is short for **Inverse Document Frequency**.\n",
    "  It is the frequency of documents that contain a specific term.\n",
    "  If a term exists in every single document, then the Document Frequency is the largest and is 1.\n",
    "  And the Inverse Document Frequency will be the smallest. In the situation, this term is non-informative for classifying the documents. The IDF is a measure of the relevance of a a term.\n",
    "  The higher the IDF is, the more relevant the term is.\n",
    "* TF-IDF is the product of TF and IDF.\n",
    "  A high TF-IDF is obtained when the Term Frequency is high and the Document Frequency is low(IDF is high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d027efc-6a82-4f26-bee8-b8e88a34afdb",
   "metadata": {},
   "source": [
    "# Term Frequency, HashingTF and CountVectorizer\n",
    "Pyspark has two functions to calculate term frequencies from documents: \n",
    "the **`HashingTF()`** and the **`CountVectorizer()`**. These two functions do two things:\n",
    "1. Indexing terms: converting words to numbers.\n",
    "2. Calculate term frequencies for each documents. \n",
    "\n",
    "The `HashingTF()` utilizes the Murmurhash 3 functions to map a raw feature(a term) into an index(a number).\n",
    "Hashing is the process of transforming data of arbitrary size to size-fixed, usually shorter data.\n",
    "The term frequencies are calculated based on the generated indices.\n",
    "For the HashingTF() method, the mapping process is very cheap. Becaues each term-to-index mapping is independent of other term-to-index mapping. The hashing function takes a unique input and generate a \"unique result\". However, **hashing collision** may occur, which means different features(terms) may be hashed to the same index.\n",
    "\n",
    "The **`CountVectorizer()`** indexes terms by descending order of term frequencies in the entire corpus, NOT the term frequencies in the document. After the indexing process, the term frequencies are calculated by documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda95f6-40f4-4a9e-9e0d-21a306325c2c",
   "metadata": {},
   "source": [
    "## Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365c33ff-78d8-47f8-8a52-f9c587da5cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|terms                                      |\n",
      "+-------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|\n",
      "|[I, love, spark, very, very, much]         |\n",
      "|[everyone, should, use, spark]             |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'terms': [\n",
    "            ['spark', 'spark', 'spark', 'is', 'awesome', 'awesome'],\n",
    "            ['I', 'love', 'spark', 'very', 'very', 'much'],\n",
    "            ['everyone', 'should', 'use', 'spark']\n",
    "        ]\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8af23-83d6-4548-9cbe-d352e83a01fa",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "The **numFeatures** parameter takes an integer, which should be larger than the total number of terms in the corpus. And it should be a power of two so that features are mapped evenly to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edd236a-29e9-4e19-ab1b-6f18e2f8eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashtf = HashingTF(numFeatures=pow(2,4), inputCol='terms', \\\n",
    "                   outputCol='features(numFeatures), [index], [term frequency]')\n",
    "stages = [hashtf]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d0e1fc-c2c9-4213-a6f0-3bae77793d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|terms                                      |features(numFeatures), [index], [term frequency]|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(16,[6,9],[3.0,3.0])                            |\n",
      "|[I, love, spark, very, very, much]         |(16,[0,6,8,12],[1.0,1.0,2.0,2.0])               |\n",
      "|[everyone, should, use, spark]             |(16,[5,6,13],[1.0,1.0,2.0])                     |\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1e709-e15e-4220-ba32-e1153d7274fe",
   "metadata": {},
   "source": [
    "You may note that the first document has three distinct terms, but only two term frequencies are obtained. This apparent discrepancy is due to a **hashing collision**: both `spark` and `is` are getting hashed to `1`. The term frequency for index `1` in the first document is `4.0` corresponding to the three counts of `spark` and the one count of `is`. The likelihood of a hashing collision can be reduced by increasing the `numFeatures` parameter passed to the `HashingTF` function(the default for example is $2^{18} = 262,144$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e95cb3-8531-472b-ba7a-c2ed2ccddedd",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "The **`CountVectorizer()`** function has three parameters to control which terms will be kept as features.\n",
    "* minTF: features that has term frequency less than minTF will be removed. If minTF=1, then no features will be removed.\n",
    "* minDF: features that has document frequency less than minDF will be removed. If minDF=1, then no features will be removed.\n",
    "* vocabSize: keep terms of the top vocabSize frequencies.\n",
    "In the example below, the `minTF=1.0, minDF=1` and `vocabSize=20`, which is larger than the total number of terms. Therefore, all features(terms) will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddcb688-0f4f-47e6-9462-31d1b9213d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "countvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20, inputCol='terms', \\\n",
    "                                    outputCol='features(vocabSize), [index], [term frequency]')\n",
    "# minTF=1.0 means a term must appear at least once in a document\n",
    "# minDf=1.0 means a term must appear in at least one document\n",
    "# vocabSize=20 means only the top 20 most frequent terms will be included in the feature vector\n",
    "stages = [countvectorizer]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5207845-013e-42c1-a2ff-e85850ce2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----------------------------------------------+\n",
      "|terms                                      |features(vocabSize), [index], [term frequency]|\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(10,[0,2,3],[3.0,2.0,1.0])                    |\n",
      "|[I, love, spark, very, very, much]         |(10,[0,1,4,5,7],[1.0,2.0,1.0,1.0,1.0])        |\n",
      "|[everyone, should, use, spark]             |(10,[0,6,8,9],[1.0,1.0,1.0,1.0])              |\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bef5f-0060-4fe5-b2e0-c8c8001b8968",
   "metadata": {},
   "source": [
    "Now, lets use the StringIndexer() to index the corpus and see if the results is consistant with the CountVectorizer() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0ca14-12cf-4f47-96a6-cbd646abbd13",
   "metadata": {},
   "source": [
    "## flatMap documents so that each row has a single term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb716841-321c-4b0d-8b28-da08172e9f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   terms|\n",
      "+--------+\n",
      "|   spark|\n",
      "|   spark|\n",
      "|   spark|\n",
      "|      is|\n",
      "| awesome|\n",
      "| awesome|\n",
      "|       I|\n",
      "|    love|\n",
      "|   spark|\n",
      "|    very|\n",
      "|    very|\n",
      "|    much|\n",
      "|everyone|\n",
      "|  should|\n",
      "|     use|\n",
      "|   spark|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "# flatten the 'terms' column in each row into words, convert to a RDD, then to a DataFrame\n",
    "df_vocab = df.select('terms').rdd. \\\n",
    "            flatMap(lambda x: x[0]). \\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7a922-f183-4d21-b6a6-0a32354bba50",
   "metadata": {},
   "source": [
    "## Calculate term frequencies in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4832f905-63a7-470c-a891-852f18e6a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      term|frequency|\n",
      "+----------+---------+\n",
      "|   {spark}|        5|\n",
      "| {awesome}|        2|\n",
      "|    {very}|        2|\n",
      "|      {is}|        1|\n",
      "|       {I}|        1|\n",
      "|    {love}|        1|\n",
      "|    {much}|        1|\n",
      "|{everyone}|        1|\n",
      "|  {should}|        1|\n",
      "|     {use}|        1|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_freq = df_vocab.rdd.countByValue()          # count the frequency of each term in df_vocab\n",
    "pdf = pd.DataFrame({\n",
    "        'term': list(vocab_freq.keys()),\n",
    "        'frequency': list(vocab_freq.values())\n",
    "    })                                            # convert the count dict to a DataFrame\n",
    "pdf\n",
    "tf = spark.createDataFrame(pdf).orderBy('frequency', ascending=False)\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebde47-b8aa-4c7a-bc74-21332d476bc6",
   "metadata": {},
   "source": [
    "## Apply StringIndexer() to df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea02aeca-fc78-4c83-a0ee-d5a3e3d30e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54195cd4-dde1-489d-b9c7-d8b5a02d92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   terms|StringIndexer(index)|\n",
      "+--------+--------------------+\n",
      "|   spark|                 0.0|\n",
      "| awesome|                 1.0|\n",
      "|    very|                 2.0|\n",
      "|       I|                 3.0|\n",
      "|everyone|                 4.0|\n",
      "|      is|                 5.0|\n",
      "|    love|                 6.0|\n",
      "|    much|                 7.0|\n",
      "|  should|                 8.0|\n",
      "|     use|                 9.0|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create an index for each unique term in 'terms' and sort\n",
    "stringindexer.fit(df_vocab).transform(df_vocab). \\\n",
    "    distinct(). \\\n",
    "    orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af64b91-8145-4b3d-bf84-c09836b6dc8c",
   "metadata": {},
   "source": [
    "The indexing result is consistant for the first three terms. The rest of terms have the same frequency which is 1. These terms can not be sorted by frequency. This might be the reason that their indices don't match the results from the CountVectorizer() method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
